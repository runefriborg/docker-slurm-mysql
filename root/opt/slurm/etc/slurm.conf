ControlMachine=localhost

#
#MailProg=/bin/mail 

# TODO: need to figure out how to get mpi working
MpiDefault=none
#MpiParams=ports=#-# 

ProctrackType=proctrack/linuxproc
ReturnToService=1
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldPort=6810-6817
SlurmctldDebug=2
SlurmdPidFile=/var/run/slurm/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/spool/slurmd
SlurmdDebug=2
SlurmUser=root
#SlurmdUser=root 
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none

# Priorities taken from slurm example:
# Activate the Multi-factor Job Priority Plugin with decay
PriorityType=priority/multifactor
# 2 week half-life
PriorityDecayHalfLife=14-0
# The larger the job, the greater its job size priority.
PriorityFavorSmall=NO
# The job's age factor reaches 1.0 after waiting in the
# queue for 2 weeks.
PriorityMaxAge=14-0
# This next group determines the weighting of each of the
# components of the Multi-factor Job Priority Plugin.
# The default value for each of the following is 1.
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=0 # don't use the qos factor

#
#
# TIMERS 
#KillWait=30 
MinJobAge=30
MaxJobCount=100000
SlurmdTimeout=1200
MessageTimeout=60
# 
# 
# SCHEDULING 
FastSchedule=1
SchedulerType=sched/backfill
#SchedulerPort=7321 
SelectType=select/cons_res
SelectTypeParameters=CR_CPU_MEMORY
SchedulerParameters=defer
# 
# 
# LOGGING AND ACCOUNTING 
ClusterName=Cluster0

# Accounting is enabled, uses a slurmdbd for storage and enforces that every
# user/job has an associated account
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStorageEnforce=associations,limits

#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherParams=NoShare

# The prolog and epilog scripts make sure the audit system is running, creates
# a scratch folder for the job and adds/removes them to the list of people
# allowed to ssh directly in to the machine.

#PrologSlurmctld=/opt/slurm/scripts/controller-prolog

#TaskProlog=/opt/slurm/scripts/slurm-task-prolog
#TaskEpilog=/opt/slurm/scripts/slurm-task-epilog

#Prolog=/opt/slurm/scripts/slurm-prolog
#Epilog=/opt/slurm/scripts/slurm-epilog
#PrologFlags=Alloc

# By default a job will only get 1G. This hopefully forces the users to think
# about how much memory they actually need.  There is no MaxMemPerCPU so users
# can still explicitly request more memory.
DefMemPerCPU=1024

# Enable the lua jobsubmit-plugin that adds the express partition to jobs with
# less than an hour of requested time.
JobSubmitPlugins=lua

# Set a limit on virtual memory used. This lets jobs use 5% more virtual memory
# than they asked for - so they can only use a small amount of swap before they
# are killed.
# VSizeFactor=105

# The task/cgroup plugins uses linux cgroups to contrain the jobs to only use
# what they have asked for. Using too much memory gets you killed, using too
# much cpu only hurts other processes from the same job.
#TaskPlugin=task/cgroup

# Pam is used to polyinstantiate /tmp and /dev/shm so that each user get a
# unique one. The epilog script should make sure to clean up these folders
# if the user has no more jobs running.
# More info:
#   http://tech.ryancox.net/2013/07/per-user-tmp-and-devshm-directories.html
#   /usr/share/doc/pam-1.1.1/txts/README.pam_namespace
#   google polyinstantiate
UsePAM=0

# We have some custom resource limits on the fronted servers that we don't want
# to propagate to compute nodes.
PropagateResourceLimits=NONE

# Default max array size os only around a thousand, quite limiting and now we
# have actually seen users "in the wild" using job arrays -- with the ricopili
# stuff in particular.
MaxArraySize=50000

# 
# COMPUTE NODES 
NodeName=DEFAULT CPUs=1 RealMemory=65536 State=UNKNOWN
NodeName=docker.example.com
PartitionName=normal Nodes=docker.example.com Default=YES MaxTime=1440 State=UP
PartitionName=express Nodes=docker.example.com Default=YES MaxTime=1440 State=UP
